{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic Dataset with AWS - Arthur Birate Kabanza "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When utilizing Amazon SageMaker for model training in machine learning workflows, the code first prepares the Titanic dataset by eliminating unnecessary columns. It then divides the dataset into training, testing, and validation sets and uploads each set to Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./screenshots/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code creates a SageMaker XGBoost model, sets up its data channels and hyperparameters, and trains the model using the provided data. The outputs of the trained model will be kept in the designated S3 location, prepared for inference and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./screenshots/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code evaluates the test dataset's form, deploys a SageMaker XGBoost model as an endpoint for predictions, and chooses a particular row for additional examination. You can utilize the deployed model (xgb_predictor) to generate predictions on the particular row (DataFrame) that contains the relevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./screenshots/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code first prepares the model endpoint by cleaning it up and then translates a particular row of data to CSV format and sends it to the deployed XGBoost model for predictions. Along with removing the target variable, it also produces a batch of input data (batch_X) for prediction-making and shows the first 20 rows for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./screenshots/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code uses the SageMaker XGBoost model to build up and execute a batch inference job. It sets up the transformer for batch inference, uploads the input data, designates the output destination, and waits for the batch inference operation to complete. The 'batch-out' location in the designated S3 bucket contains the batch inference results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./screenshots/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the code retrieves batch inference results from an S3 object, converts continuous predictions into binary predictions using a specified threshold, and prints the top rows of both the binary predictions and the original test data to compare the model's predictions with the actual input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./screenshots/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this code evaluates the performance of a binary classification model by calculating the ROC AUC, accuracy, and displaying a ROC curve to visualize the model's performance in terms of true positive and false positive rates. The confusion matrix elements are also printed to provide a detailed breakdown of the model's classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./screenshots/8.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
